{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c793632b",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "# ML Workflow Demo: Kubeflow - Katib - MLFlow\n",
    "\n",
    "## Overview\n",
    "\n",
    "This guide intended to introduce end users to complete ML workflow using Kubeflow. In particular, examples of Kubeflow pipelines using Katib hyperparameter tuning and MLFlow model registry are presented along with some common pipeline steps and interfaces such as S3.\n",
    "\n",
    "The following diagram outlines ML workflow presented in this guide. Major pipeline steps include:\n",
    "- Ingestion of dataset.\n",
    "- Cleaning up the dataset.\n",
    "- Store of cleaned data to S3 bucket.\n",
    "- Hyperparameter tuning using Katib and Tensorflow training container image (with MLFlow store functionality).\n",
    "- Converting Katib tuning results to streamlined format.\n",
    "- Model training using best parameters from tuning stage.\n",
    "- Storing the resulting production model to MLFlow model registry.\n",
    "\n",
    "![Diagram](./images/ML-Workflow-Demo-Diagram.png)\n",
    "\n",
    "This repository contains all artifacts needed to support this guide. `images/` directory contains all related screenshorts and diagrams. `resources/` directory contains Jupyter notebook containing all steps in this guide, `Dockerfile` and Python script for training image used in this guide.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Deployed Kubeflow instance including Katib, and access to Kubeflow dashboard. For sample Kubeflow deployment refer to https://charmed-kubeflow.io/docs/quickstart (note: the `kubeflow-lite` bundle does not include Katib - `juju deploy kubeflow --trust` instead when you get to that step)\n",
    "- Deployed MLFlow. For deployment of Charmed MLFlow refer to https://charmed-kubeflow.io/docs/mlflow\n",
    "- Familiarity with Python, Docker, Jupyter notebooks.\n",
    "\n",
    "## Instructions\n",
    "The following are the instructions that outline the workflow process.\n",
    "\n",
    "1. Access Kubeflow dashboard via URL, eg http://10.64.140.43.nip.io/\n",
    "\n",
    "2. Navigate to Notebooks.\n",
    "\n",
    "3. Create a new notebook.\n",
    "    1. Fill in name\n",
    "    2. Select Tensorflow image `jupyter-tensorflow-full:v1.6.0`\n",
    "    3. Select minimum configuration: 1 CPU and 4GB of RAM\n",
    "\n",
    "![NotebookCreate](./images/ML-Workflow-NotebookCreate-diag.png)\n",
    "\n",
    "![NewNotebook](./images/ML-Workflow-NewNotebook-diag.png)\n",
    "\n",
    "4. Connect to the newly created notebook.\n",
    "\n",
    "5. Create a Jupyter notebook to hold code that will specify the Kubeflow pipeline.\n",
    "\n",
    "![NewJupyterNotebook](./images/ML-Workflow-NewJupyterNotebook-diag.png)\n",
    "\n",
    "NOTE: The following Jupyter notebook contains all the steps outlined below: [https://github.com/canonical/kubeflow-examples/ml-workflow-demo-kfp-katib-mlflow/ml-workflow-demo-kfp-katib-mlflow.ipynb](./ml-workflow-demo-kfp-katib-mlflow.ipynb)\n",
    "\n",
    "6. To setup environment add the following cells to the newly created Jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2ac9f9-3a58-4b56-867a-d84c88de61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp==1.8.12\n",
    "!pip install kubeflow-katib==0.13.0\n",
    "\n",
    "import json\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp import Client\n",
    "from kfp import components\n",
    "from kfp.onprem import use_k8s_secret\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff\n",
    "from kubeflow.katib import ApiClient\n",
    "from kubeflow.katib import V1beta1ExperimentSpec\n",
    "from kubeflow.katib import V1beta1AlgorithmSpec\n",
    "from kubeflow.katib import V1beta1ObjectiveSpec\n",
    "from kubeflow.katib import V1beta1ParameterSpec\n",
    "from kubeflow.katib import V1beta1FeasibleSpace\n",
    "from kubeflow.katib import V1beta1TrialTemplate\n",
    "from kubeflow.katib import V1beta1TrialParameterSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ae698",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "7. Create pipeline steps that will do data ingestion and cleanup. Setup transfer of clean data to the next step using S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af5bc1-0b99-4d8e-8c8f-9818634c7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ingest operation.\n",
    "# Output is in outputs['data']\n",
    "ingest_data_op = components.load_component_from_url(\n",
    "'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/contrib/web/Download/component.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21278f09-7938-419e-a9d0-d57ad3952b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data clean up operation.\n",
    "def clean_arff_data(\n",
    "    bucket,\n",
    "    key,\n",
    "    input_file: components.InputPath(str)\n",
    ") -> str:\n",
    "    import pandas as pd\n",
    "    import boto3\n",
    "    import os\n",
    "    from io import StringIO\n",
    "    from scipy.io.arff import loadarff\n",
    "\n",
    "    print(f\"Loading input file {input_file}\")\n",
    "\n",
    "    # Convert to dataframe arff format.\n",
    "    raw_data = loadarff(input_file)\n",
    "    df_data = pd.DataFrame(raw_data[0].copy())\n",
    "    print(f\"Loaded data file of shape {df_data.shape}\")\n",
    "\n",
    "    # Convert target column to numeric.\n",
    "    df_data.iloc[:, -1] = pd.get_dummies(df_data['CHURN']).iloc[:, 0]\n",
    "\n",
    "    # Remove missing values.\n",
    "    df_clean = df_data.dropna(axis=1)\n",
    "    df_clean.loc[:,'CHURN']=pd.get_dummies(df_data['CHURN']).iloc[:, 0]\n",
    "\n",
    "    # Get rid of non-numeric columns.\n",
    "    df_clean = df_clean.select_dtypes(exclude='object')\n",
    "\n",
    "    # Save results to S3\n",
    "    csv_buffer = StringIO()\n",
    "    df_clean.to_csv(csv_buffer)\n",
    "    s3_resource = boto3.resource(\n",
    "        's3',\n",
    "        endpoint_url='http://minio.kubeflow.svc.cluster.local:9000',\n",
    "        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "    )\n",
    "    check_bucket = s3_resource.Bucket(bucket)\n",
    "    if not check_bucket.creation_date:\n",
    "        # bucket does not exist, create it\n",
    "        s3_resource.create_bucket(Bucket=bucket)\n",
    "    print(f\"Saving CSV of shape {df_clean.shape} to s3\")\n",
    "    s3_resource.Object(bucket, key).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "    return \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24208ef",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "8. Create the next pipeline step that will do hyperparameter tuning using Katib and a training container image `docker.io/misohu/kubeflow-training:latest`. For more details on the training container image refer to [resources README](./resources/README.md) of this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397ddc8-8e6f-438a-a014-bbd45aaed5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Katib hyperparameter tuning operation.\n",
    "def create_katib_experiment_op(experiment_name, experiment_namespace, bucket, key):\n",
    "    # Trial count specification.\n",
    "    max_trial_count = 5\n",
    "    max_failed_trial_count = 3\n",
    "    parallel_trial_count = 2\n",
    "\n",
    "    # Objective specification.\n",
    "    objective = V1beta1ObjectiveSpec(\n",
    "        type=\"maximize\",\n",
    "        goal=0.95,\n",
    "        objective_metric_name=\"accuracy\"\n",
    "    )\n",
    "\n",
    "    # Algorithm specification.\n",
    "    algorithm = V1beta1AlgorithmSpec(\n",
    "        algorithm_name=\"random\",\n",
    "    )\n",
    "\n",
    "    # Experiment search space.\n",
    "    # In this example we tune the number of epochs.\n",
    "    parameters = [\n",
    "        V1beta1ParameterSpec(\n",
    "            name=\"epochs\",\n",
    "            parameter_type=\"int\",\n",
    "            feasible_space=V1beta1FeasibleSpace(\n",
    "                min=\"5\",\n",
    "                max=\"10\"\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Experiment trial template.\n",
    "    trial_spec = {\n",
    "        \"apiVersion\": \"kubeflow.org/v1\",\n",
    "        \"kind\": \"TFJob\",\n",
    "        \"spec\": {\n",
    "            \"tfReplicaSpecs\": {\n",
    "                \"Chief\": {\n",
    "                    \"replicas\": 1,\n",
    "                    \"restartPolicy\": \"OnFailure\",\n",
    "                    \"template\": {\n",
    "                        \"metadata\": {\n",
    "                            \"annotations\": {\n",
    "                                \"sidecar.istio.io/inject\": \"false\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"spec\": {\n",
    "                            \"containers\": [\n",
    "                                {\n",
    "                                    \"name\": \"tensorflow\",\n",
    "                                    \"image\": \"docker.io/misohu/kubeflow-training:latest\",\n",
    "                                    \"command\": [\n",
    "                                        \"python\",\n",
    "                                        \"/opt/model.py\",\n",
    "                                        \"--s3-storage=true\",\n",
    "                                        \"--epochs=${trialParameters.epochs}\",\n",
    "                                        f\"--bucket={bucket}\",\n",
    "                                        f\"--bucket-key={key}\",\n",
    "                                    ],\n",
    "                                    \"envFrom\": [\n",
    "                                      {\n",
    "                                        \"secretRef\": {\n",
    "                                          \"name\": \"mlpipeline-minio-artifact\"\n",
    "                                        }\n",
    "                                      }\n",
    "                                    ]\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "               \"Worker\": {\n",
    "                    \"replicas\": 1,\n",
    "                    \"restartPolicy\": \"OnFailure\",\n",
    "                    \"template\": {\n",
    "                        \"metadata\": {\n",
    "                            \"annotations\": {\n",
    "                                \"sidecar.istio.io/inject\": \"false\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"spec\": {\n",
    "                            \"containers\": [\n",
    "                                {\n",
    "                                    \"name\": \"tensorflow\",\n",
    "                                    \"image\": \"docker.io/misohu/kubeflow-training:latest\",\n",
    "                                    \"command\": [\n",
    "                                        \"python\",\n",
    "                                        \"/opt/model.py\",\n",
    "                                        f\"--s3-storage=true\",\n",
    "                                        \"--epochs=${trialParameters.epochs}\",\n",
    "                                        f\"--bucket={bucket}\",\n",
    "                                        f\"--bucket-key={key}\",\n",
    "                                    ],\n",
    "                                    \"envFrom\": [\n",
    "                                      {\n",
    "                                        \"secretRef\": {\n",
    "                                          \"name\": \"mlpipeline-minio-artifact\"\n",
    "                                        }\n",
    "                                      }\n",
    "                                    ]\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Configure parameters for the Trial template.\n",
    "    trial_template = V1beta1TrialTemplate(\n",
    "        primary_container_name=\"tensorflow\",\n",
    "        trial_parameters=[\n",
    "            V1beta1TrialParameterSpec(\n",
    "                name=\"epochs\",\n",
    "                description=\"Learning rate for the training model\",\n",
    "                reference=\"epochs\"\n",
    "            )\n",
    "        ],\n",
    "        trial_spec=trial_spec\n",
    "    )\n",
    "\n",
    "    # Create an Experiment from the above parameters.\n",
    "    experiment_spec = V1beta1ExperimentSpec(\n",
    "        max_trial_count=max_trial_count,\n",
    "        max_failed_trial_count=max_failed_trial_count,\n",
    "        parallel_trial_count=parallel_trial_count,\n",
    "        objective=objective,\n",
    "        algorithm=algorithm,\n",
    "        parameters=parameters,\n",
    "        trial_template=trial_template\n",
    "    )\n",
    "\n",
    "    # Create the KFP operation for the Katib experiment.\n",
    "    # Experiment spec should be serialized to a valid Kubernetes object.\n",
    "    katib_experiment_launcher_op = components.load_component_from_url(\n",
    "        \"https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kubeflow/katib-launcher/component.yaml\")\n",
    "    op = katib_experiment_launcher_op(\n",
    "        experiment_name=experiment_name,\n",
    "        experiment_namespace=experiment_namespace,\n",
    "        experiment_spec=ApiClient().sanitize_for_serialization(experiment_spec),\n",
    "        experiment_timeout_minutes=60,\n",
    "        delete_finished_experiment=False)\n",
    "\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3af79b-d70f-4703-b945-a4fd1cd05e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Katib experiment hyperparameter results to arguments in string format.\n",
    "def convert_katib_results(katib_results) -> str:\n",
    "    import json\n",
    "    import pprint\n",
    "    katib_results_json = json.loads(katib_results)\n",
    "    print(\"Katib hyperparameter tuning results:\")\n",
    "    pprint.pprint(katib_results_json)\n",
    "    best_hps = []\n",
    "    for pa in katib_results_json[\"currentOptimalTrial\"][\"parameterAssignments\"]:\n",
    "        if pa[\"name\"] == \"epochs\":\n",
    "            best_hps.append(\"--epochs=\" + pa[\"value\"])\n",
    "    print(\"Best hyperparameters: {}\".format(best_hps))\n",
    "    return \" \".join(best_hps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0961a5d",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "9. Create the last step of the pipeline that will do model training using Tensorflow based on Katib tuning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705716a-46e7-4eb3-b52b-dfecb9a292c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow job operation.\n",
    "def create_tfjob_op(tfjob_name, tfjob_namespace, model, bucket, key):\n",
    "    tf_model = str(model)\n",
    "\n",
    "    tfjob_chief_spec = {\n",
    "        \"replicas\": 1,\n",
    "        \"restartPolicy\": \"OnFailure\",\n",
    "        \"template\": {\n",
    "            \"metadata\": {\n",
    "                \"annotations\": {\n",
    "                    \"sidecar.istio.io/inject\": \"false\"\n",
    "                }\n",
    "            },\n",
    "            \"spec\": {\n",
    "                \"containers\": [\n",
    "                    {\n",
    "                        \"name\": \"tensorflow\",\n",
    "                        \"image\": \"docker.io/misohu/kubeflow-training:latest\",\n",
    "                        \"command\": [\n",
    "                            \"python\",\n",
    "                            \"/opt/model.py\",\n",
    "                            \"--s3-storage=true\",\n",
    "                            f\"{tf_model}\",\n",
    "                            \"--mlflow-model-name=ml-workflow-demo-model\",\n",
    "                            f\"--bucket={bucket}\",\n",
    "                            f\"--bucket-key={key}\",\n",
    "                        ],\n",
    "                        \"envFrom\": [\n",
    "                          {\n",
    "                            \"secretRef\": {\n",
    "                              \"name\": \"mlpipeline-minio-artifact\"\n",
    "                            }\n",
    "                          }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    tfjob_worker_spec = {\n",
    "        \"replicas\": 1,\n",
    "        \"restartPolicy\": \"OnFailure\",\n",
    "        \"template\": {\n",
    "            \"metadata\": {\n",
    "                \"annotations\": {\n",
    "                    \"sidecar.istio.io/inject\": \"false\"\n",
    "                }\n",
    "            },\n",
    "            \"spec\": {\n",
    "                \"containers\": [\n",
    "                    {\n",
    "                        \"name\": \"tensorflow\",\n",
    "                        \"image\": \"docker.io/misohu/kubeflow-training:latest\",\n",
    "                        \"command\": [\n",
    "                            \"python\",\n",
    "                            \"/opt/model.py\",\n",
    "                            \"--s3-storage=true\",\n",
    "                            f\"{tf_model}\",\n",
    "                            f\"--bucket={bucket}\",\n",
    "                            f\"--bucket-key={key}\",\n",
    "                        ],\n",
    "                        \"envFrom\": [\n",
    "                          {\n",
    "                            \"secretRef\": {\n",
    "                              \"name\": \"mlpipeline-minio-artifact\"\n",
    "                            }\n",
    "                          }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create the KFP task for the TFJob.\n",
    "    tfjob_launcher_op = components.load_component_from_url(\n",
    "\"https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kubeflow/launcher/component.yaml\")\n",
    "    op = tfjob_launcher_op(\n",
    "        name=tfjob_name,\n",
    "        namespace=tfjob_namespace,\n",
    "        chief_spec=json.dumps(tfjob_chief_spec),\n",
    "        worker_spec=json.dumps(tfjob_worker_spec),\n",
    "        tfjob_timeout_minutes=60,\n",
    "        delete_finished_tfjob=False)\n",
    "    return op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bc1e86",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "10. Define a complete pipeline that consists of all steps created earlier. Note that the name of the pipeline must be uqinue. If there was previously defined pipeline with the same name and within the same namespace either change the name of current pipeline or delete the older pipeline from the namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f18d3-1cd0-4542-9726-7e50565ad752",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"demo-pipeline\"\n",
    "namespace = \"admin\"\n",
    "s3_bucket = \"demo-dataset\"\n",
    "key = \"data.csv\"\n",
    "dataset_url = \"https://www.openml.org/data/download/53995/KDDCup09_churn.arff\"\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name = \"ML Workflow in Kubeflow\",\n",
    "    description = \"Demo pipeline\"\n",
    ")\n",
    "def demo_pipeline(name=name, namespace=namespace):\n",
    "\n",
    "    # Step 1: Download dataset.\n",
    "    ingest_data_task = ingest_data_op(url=dataset_url)\n",
    "\n",
    "    # Step 2: Clean up the dataset and store it in S3 bucket.\n",
    "    clean_data_op = components.create_component_from_func(\n",
    "            clean_arff_data,\n",
    "            \"clean_data.yaml\",\n",
    "            packages_to_install=[\"pandas==1.2.4\", \"scipy==1.7.0\", \"boto3\"],\n",
    "        )\n",
    "    clean_data_task = (clean_data_op(\n",
    "        s3_bucket,\n",
    "        key,\n",
    "        ingest_data_task.outputs['data']\n",
    "    ).apply(use_k8s_secret(\n",
    "        secret_name='mlpipeline-minio-artifact',\n",
    "        k8s_secret_key_to_env={\n",
    "            'accesskey': 'AWS_ACCESS_KEY_ID',\n",
    "            'secretkey': 'AWS_SECRET_ACCESS_KEY',\n",
    "        })))\n",
    "\n",
    "    with dsl.Condition(clean_data_task.output == \"Done\"):\n",
    "        # Step 3: Run hyperparameter tuning with Katib.\n",
    "        katib_op = create_katib_experiment_op(name, namespace, s3_bucket, key)\n",
    "\n",
    "        # Step 4: Convert Katib results produced by hyperparameter tuning to model.\n",
    "        convert_katib_results_op = components.func_to_container_op(convert_katib_results)\n",
    "        best_katib_model_op = convert_katib_results_op(katib_op.output)\n",
    "\n",
    "        # Step 5: Run training with TFJob. Model will be stored into ML Flow model registry\n",
    "        # (done inside container image).\n",
    "        tfjob_op = create_tfjob_op(name, namespace, best_katib_model_op.output, s3_bucket, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59a8bf",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "11. Execute pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131bb2ee-9b2b-4b81-8e28-6b86542ce531",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_client = Client()\n",
    "run_id = kfp_client.create_run_from_pipeline_func(\n",
    "        demo_pipeline,\n",
    "        namespace=namespace,\n",
    "        arguments={},\n",
    "    ).run_id\n",
    "print(f\"Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15017aa4",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "12. Observe run details by selecting **Run details** link.\n",
    "\n",
    "![Run](./images/ML-Workflow-RunDetails.png)\n",
    "\n",
    "![Pipeline](./images/ML-Workflow-Pipeline.png)\n",
    "\n",
    "13. Verify that model is stored in MLFlow model registry by navigating to ML Flow dashboard, eg. http://10.64.140.43.nip.io/mlflow/#/\n",
    "\n",
    "![MLFlow](./images/ML-Workflow-MLFLowRegistry.png)\n",
    "\n",
    "14. Now model is ready to be deployed!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b9fcc3d702a1257c32d40ffb9a24ef331338086f3e2806172e9fc05eb0bda1b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
